---
title: 'HW1: Regression Modeling'
author: "Brian"
date: "2022.12.17"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Before working on HW1, you better complete the practice of ISLR ch2-3 R Labs. In particular, you may follow similar fitting steps demonstrated in ISLR ch3 R Lab for the following two data analysis.


### Problem1

The first data consist of 4 variables (daily measurements) measured at the same place: 

* ozone: response variable
* radiation
* temperature 
* wind (speed)

**Goal:** Predict ozone using regression models.

Your analysis should include the following:

(a) Exploratory data analysis (EDA) among 4 variables
(b) Regression model fitting and model summaries.
(c) Model selection and diagnostics
(d) Comments on your prediction results and scientific findings. (state at least 3 viewpoints with data evidence)

## a. EDA  
```{r}
dat1 <- read.csv("ozone.csv")
head(dat1)
dim(dat1)

pairs(dat1)
```
  
  From the pair plot, we can find that the response variable ozone seems to have some positive correlation with radiation and temperature, while wind seems to have a negative correlation with ozone. Otherwise, temperature also seems to have negative correlation with wind. And radiation seems to have no correlation with other predictors. Moreover, both radiation and wind seem to have non-linear relationship with ozone.
```{r}
par(mfrow=c(2,2))
hist(dat1$ozone)
hist(dat1$radiation)
hist(dat1$temperature)
hist(dat1$wind)
```
  
  From the histograms above, we can observe that the distribution of ozone is long-tailed and skewed to the right. And there is a data point that has a unusual high ozone value that we need to put an eye on. The histogram of radiation seems to be uniformly distributed. And the histogram of temperature and wind seems to be normally distributed but we might need more analysis on the predictor wind since it looks quite different from normal.
```{r}
par(mfrow=c(2,2))
boxplot(dat1$ozone, main="Boxplot of ozone")
boxplot(dat1$radiation, main="Boxplot of radiation")
boxplot(dat1$temperature, main="Boxplot of temperature")
boxplot(dat1$wind, main="Boxplot of wind")
```
  
  From the box plots above, we can find that there are 2 outliers in the response ozone and 3 in the predictor wind. Although the number of outliers is small, we still need to be aware of the effect they have on the model.
  
## b. Model fitting
```{r}
md1 <- lm(ozone~., data = dat1)
summary(md1)
```
  
  After fitting the full model, we can get a model with $R^2=0.606$. The model has a F-statistic with small p-value, which means at least 1 of 3 predictors is important. And for the 3 predictors, they are all  significance under 0.05 significant level, so all predictors are significant while all of them are in the model.

## c. Model Selection and Diagnostics
```{r}
library(car)
par(mfrow=c(2,2))
plot(md1)
car::vif(md1)
dat1[c(7,30,77),]
```
  
  The residual vs fitted plot indicates that there are both non-constant variance and some problems in the mean structure that we need to fix.
  
  The Q-Q plot shows that the data does not lie in the Q-Q line slightly, indicates that it seems to violate the normality assumption. We still need to look at other plots to know whether we need to fix it.
  
  The line in the scale-location plot is not horizontal at all. Also, the data does not seem to spread equally around the line in the left part of the plot. This means there might be some non-constant variance exist in our model.
  
  The line in the residual vs leverage plot also shows similar result as the scale-location plot does.
  
  The vif value shows that there are no collinearity exists in the model since all vif values are about 1.
  
  Also, there are some data points having high residuals or leverage, which have big impact on our model.
  
  Data 7 and 30 have high leverage, and also having residual about 2, that might have huge impact on our model. By observing their value we can find that they both have wind > 20, which is really high in this data. 
  
  Data 77 has high residual value, which is an outlier under our model. It has a 168 ozone value, which is surprisingly high in our data. But since the other variables' value are quite normal, it's hard to determine the reason of the outcome.
  
```{r}
dat1_ol <- dat1[-c(77),]
md1_1 <- lm(ozone~., data = dat1_ol)
summary(md1_1)
par(mfrow=c(2,2))
plot(md1_1)
```
  
  By excluding the outlier, we have got a better fit with $R^2=0.637$, which is a big improvement from the previous model. We can try to exclude the outlier in our later model if the performance is still affected huge by it.
```{r}
library(MASS)
boxcox(md1, plotit=T, lambda=seq(0,1,by=0.1))
md1_2 <- lm(ozone^0.25~., data = dat1)
summary(md1_2)
par(mfrow=c(2,2))
plot(md1_2)
```
  
  Since we find that we need to fix the problem of unequal variance, we can do some transformation to our response. Here we use box-cox method to find the most suitable transformation, and find out $\lambda=0.25$ might be a good choice. By taking a fourth root of the response, the Q-Q line now matches the data, which means we have fix the normality problem.
  
  After transforming the response, we still need to fix the problem of the mean structure of model. Since we find radiation and wind may have non-linear relationship with ozone in our EDA, we can try to add some 2nd degree term to improve the fit.
```{r}
md1_3 <- update(md1_2, .~.+I(radiation^2))
summary(md1_3)
par(mfrow=c(2,2))
plot(md1_3)
```
  
  Adding the 2nd degree term of radiation gives a little improvement of model. But the new term is not significant in our model, and the result of residual plots doesn't change much. So we can keep the result and try to add other terms. 
```{r}
md1_4 <- update(md1_2, .~.+I(wind^2))
summary(md1_4)
par(mfrow=c(2,2))
plot(md1_4)
```
  
  Adding the 2nd degree term of wind improves the model a lot. The predictors are all significant and the $R^2$ increases a lot. Also, the lines in all residual plots seems to be more horizontal, that shows the problems we find in the original model have been revised well.
```{r}
md1_5 <- update(md1_2, .~.+I(wind^2)+I(radiation^2))
summary(md1_5)
par(mfrow=c(2,2))
plot(md1_5)
```
  Adding both the 2nd degree term of radiation and wind gives a similar, or even a worse result as just adding the 2nd degree term of wind. To keep the model simple, we will remain the model of just adding the 2nd degree term of wind.
  
  Although there are still some high leverage points, they now have small residuals that are less influential to our model. Also, the outlier we found in the original model now is not an outlier now in the new model. Which means there's no need for us to remove any data point to get avoid our model get heavily impact by influential points.
  
  So our final model is obtained: $ozone^{1/4}=radiation+temperature+wind+wind^2$.
  
## d. Prediction results and findings
  
  * All 3 predictors are important factors for the ozone value. Radiation and temperature are positive correlated to ozone while wind is negative correlated. Although the term of $wind^2$ is positive, but it has coefficient 30 times smaller than wind.  While our value of wind is only in the range 0-20, the 2nd degree slows down the increment of ozone when the value of wind increases and does not change the negative correlation of ozone and wind.
  
  * Our response ozone may not fit the normality assumption in linear model. Since we have add a 0.25 degree, it might means the variance of ozone $\propto (E(ozone))^{3/2}$. After the transformation, we can see that the distribution looks more like a normal distribution than the original data.
```{r}
hist((dat1$ozone)^0.25)
```
 
  * Our model seems to have some problem with those data having big observed values. We have find the pattern for those data having normal value, but when it comes to unusual data, we still can't handle them well. And the model tends to underestimate since most data point is under the line $y=x$.
```{r}
pre_result_1 <- predict(md1_4, dat1, interval = "prediction")

plot(dat1$ozone, (pre_result_1[,1])^4, ylim=c(0,150), col=2, pch=16, ylab="predicted values", xlab="observed values")
curve(x^1, from=min(dat1$ozone), to=max(dat1$ozone), col=4, add=T)
legend("topleft", legend=c("predicted value"), col=c("red"), lty=c(0), pch=c(16), lwd=2)
```

---------------------
### Problem2

The second problem concerns about the Prostate data (more data descriptions can be found in ESL book chapter 3.2.1). There are 8 input variables and 1 response variable:

* Input variables (columns 1--8)
  + lcavol
  + lweight
  + age
  + lbph
  + svi
  + lcp
  + gleason
  + pgg45

* Response variable: lpsa (column 9)

* Indicator for training set (column 10): there are 97 observations in total, in which 70 obs'n are used as training data set and the rest of 27 observations are used as validation data. **Note: our training index is different from those used in the ESL book chapter 3.2.1.**



**Goal:** Predict lpsa using regression models.

Your analysis is based on the training data set only and should include the following:

(a) EDA 
(b) Determine a good regression model for predicting lpsa 
(c) Describe the important main effects and interaction effects.
(d) Predict lpsa for the validation data set based on the fitted model, with their prediction intervals. And compared the prediction results to the true observations.  Comment on your model performance.

## a. EDA
```{r}
dat2 <- read.csv("Prostate.csv")
head(round(dat2,3))
dim(dat2)

dat2_train <- subset(dat2, train.idx %in% c(1))
dat2_test <- subset(dat2, train.idx %in% c(0))

library("corrplot")
pairs(dat2)
dat2_corr <- cor(dat2)
corrplot(dat2_corr)
```
  
  From the pairs plot, we can know that there are a lot of duplicates in the data set. Also, most of the data seems to be positive correlated or uncorrelated to the response. And there aren't predictors that have obvious non-linear relationship with response.
  
  From the correlation plot, we can know all predictors are positive correlated or uncorrelated to response. And lcavol have the highest correlation with lpsa, while lweight, svi, lcp are also highly correlated with lpsa. Also, we can divide the predictors into 2 subsets. The first one is lweight, age and lbph, having higher correlation with each other than other variables. The second is svi, lcp, gleason, pgg45 and lcavol, also having high correlations in the group. So if we create our model by using all the predictors, we might need to remove a lot of them until each group remains only 1-2 predictors to avoid strong collinearity.
  
```{r}
par(mfrow=c(2,2))
hist(dat2$lcavol)
hist(dat2$lweight)
hist(dat2$age)
hist(dat2$lbph)

par(mfrow=c(2,2))
hist(dat2$svi)
hist(dat2$lcp)
hist(dat2$gleason)
hist(dat2$pgg45)

hist(dat2$lpsa)
```
  
  The histogram of lcavol, lweight, age and lpsa shows that they seems to be normally distributed. While lbph, lcp, gleason and pgg45 shows that they are skewed to the right heavily. And svi has only 2 value, while more data has value 0 than value 1.
```{r}
par(mfrow=c(1,2))
boxplot(lpsa~svi, data = dat2, main="Boxplot of svi")
boxplot(lpsa~gleason, data = dat2, main="Boxplot of gleason")

```
  
  From the boxplot of svi to lpsa, we can know that if svi is 1, than lpsa tends to have a higher value. From the boxplot of gleason to lpsa, gleason with 6 and 7 have significance difference while 8 and 9 needs more data for more information.
col)

  
## b. Regression Model Fitting
```{r}
md2 <- lm(lpsa~.-train.idx, data = dat2_train)
summary(md2)

par(mfrow=c(2,2))
plot(md2)
```

  From the summary of the full model, we can know most predictors are not significant since they have shown high correlations in out correlation plot. Since lcavol and lweight are from different groups we divided in the previous EDA(group1: lweight, age and lbph; group2: lcavol, lcp, gleason, pgg45 and svi), they may cause other predictors to be insignificant.
  
  The residual plots of the full model shows there aren't many problems exist in our model. The Q-Q plot shows our data seems to be light-tailed instead of following the normality assumption. The plots show that our model might need include non-linear terms of predictors or interaction terms, while the response term can remain unchanged temporary.
  
  We can try variable selection by backward elimination method to get the important predictors we want.
```{r}
md2_1 <- update(md2, .~.-pgg45)
summary(md2_1)

md2_2 <- update(md2_1, .~.-gleason)
summary(md2_2)

md2_3 <- update(md2_2, .~.-lcp)
summary(md2_3)

md2_4 <- update(md2_3, .~.-lbph)
summary(md2_4)

md2_5 <- update(md2_4, .~.-age)
summary(md2_5)

par(mfrow=c(2,2))
plot(md2_5)
```

  After variable selection, we find only lcavol. lweight and svi pass the threshold of p-value = 0.05. But that doesn't mean the variables that are eliminated don't play important rolls, they may be removed for their high correlation with these predictors. Also, there may exist some interactions between variables since the scale-location plot still can be improved. The light-tailed distributed problem have also be fixed just by removing redundant predictors.
```{r}
md2_6 <- update(md2_5, .~.+I(lcavol*lweight))
summary(md2_6)

par(mfrow=c(2,2))
plot(md2_6)

md2_7 <- update(md2_5, .~.+I(svi*lweight))
summary(md2_7)

par(mfrow=c(2,2))
plot(md2_7)

md2_8 <- update(md2_5, .~.+I(svi*lweight)+I(lcavol*lweight))
summary(md2_8)

par(mfrow=c(2,2))
plot(md2_8)
```
  
  Since we have divide the predictors in 2 groups, I want to check whether there are possible interactions between these groups. After adding the interaction term, I find that if we add the interaction term of lcavol and lweight, the predictor will be significant. But adding the interaction term of svi and lweight or both can cause the interaction terms be insignificant. So I will add the interaction term of lcavol and lweight since it is significant and have improve the model well.
```{r}
vif(md2_6)
```
  
  The vif value shows that lcavol and the interaction term of lcavol and lweight have great correlation. But consider that interaction term includes lcavol and it can provide better performance, I will keep both predictors in the model.
  

## c. Important Main effects and Interaction effects
  Since the final model we got is $lpsa~lcavol+lweight+svi+lcavol*lweight$, we can know that the important main effects are lcavol, lweight and svi. And the important interaction is lcavol*lweight. But that doesn't means other predictors aren't important main effect.
```{r}
md3 <- lm(lpsa~lcp, data=dat2_train)
summary(md3)
```
  
  We can see that although lcp is not in our model, but if we fit a simple linear regression of lcp to lpsa, it can be very significant and explain about 1/3 of the data. The reason of it not being in the model is because it has a strong correlation with lcavol. So lcavol and lcp can explain similar parts of our data. And is hard to say that lcp is not an important main effect of predicting lpsa. We can only know lcavol, lweight and svi are 3 important main effects of lpsa, but not the only ones.
  
## d. Prediction Results
```{r}
pre_result <- predict(md2_6, dat2_test, interval = "prediction")

plot(dat2_test$lpsa, pre_result[,1], ylim=c(0,5), col=2, pch=16, ylab="predicted values", xlab="observed values")
curve(x^1, from=min(dat2_test$lpsa), to=max(dat2_test$lpsa), col=4, add=T)

for (i in 1:27){
  lines(rep(dat2_test$lpsa[i],2), pre_result[i, 2:3], col="gray")
}

legend("topleft", legend=c("predicted value", "95% prediction interval"), col=c("red","gray"), lty=c(0,1), pch=c(16,-1), lwd=2)

```

  The predicted value seems to be good in most points, but some predictions still have large difference with the observed value. By the prediction interval, we can know maybe we need more data point to make a more accurate prediction. While most lpsa value of data points are in the range 0 to 4, the prediction interval have a length of about 3. Although most intervals cover the line y=x, the interval is too large to be meaningful. To reduce the length of prediction interval, we need to reduce the standard error, which can be done by collecting more data.
```{r}
test_mean <- mean(dat2_test$lpsa)
test_tss <- sum((dat2_test$lpsa-test_mean)^2)
test_rss <- sum((dat2_test$lpsa-pre_result[,1])^2)
test_rsq <- 1 - test_rss/test_tss
test_rsq
```
  Calculating the $R^2$ of our prediction, we get a 0.455 $R^2$ value. The performance of our prediction is not good comparing to the $R^2=0.674$ in our training data. The reason might be the predictors we get in the data have strong correlation to each other, so the useful predictors are far less than the predictors we get. Adding new predictors when we collect data may solve this problem well.
